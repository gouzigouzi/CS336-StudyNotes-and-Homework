# Assignment 2 (systems): Systems and Parallelism

## 2 分布式数据并行训练
在本作业的第二部分，我们将探索利用多块GPU训练语言模型的方法，重点关注数据并行性。首先会介绍 PyTorch 中的分布式通信基础，随后研究分布式数据并行训练的朴素实现，并通过实现和基准测试多种优化方案来提升通信效率。

### 2.1 PyTorch中的单节点分布式通信
我们先从PyTorch中的一个简单分布式应用入手，目标是生成四个随机整数张量并计算它们的总和。

在下面的分布式场景中，我们将启动四个工作进程，每个进程生成一个随机整数张量。为了对所有工作进程中的张量求和，我们会调用全归约（**all-reduce**） 集合通信操作——该操作会用全归约结果（即总和）替换每个进程上的原始数据张量。

以下是相关代码：
```python
import os
import torch
import torch.distributed as dist
import torch.multiprocessing as mp

def setup(rank, world_size):
    os.environ["MASTER_ADDR"] = "localhost"
    os.environ["MASTER_PORT"] = "29500"
    dist.init_process_group("gloo", rank=rank, world_size=world_size)

def distributed_demo(rank, world_size):
    setup(rank, world_size)
    data = torch.randint(0, 10, (3,))
    print(f"rank {rank} 数据（全归约前）: {data}")
    dist.all_reduce(data, async_op=False)
    print(f"rank {rank} 数据（全归约后）: {data}")

if __name__ == "__main__":
    world_size = 4
    mp.spawn(fn=distributed_demo, args=(world_size, ), nprocs=world_size, join=True)
```
运行上述脚本后，输出结果如下。正如预期，每个工作进程最初持有不同的数据张量；经过全归约操作（对所有工作进程的张量求和）后，每个工作进程上的数据会被原地修改为全归约结果。
```
$ uv run python distributed_hello_world.py
rank 3 数据（全归约前）: tensor([3, 7, 8])
rank 0 数据（全归约前）: tensor([4, 4, 7])
rank 2 数据（全归约前）: tensor([6, 0, 7])
rank 1 数据（全归约前）: tensor([9, 5, 3])
rank 1 数据（全归约后）: tensor([22, 16, 25])
rank 0 数据（全归约后）: tensor([22, 16, 25])
rank 3 数据（全归约后）: tensor([22, 16, 25])
rank 2 数据（全归约后）: tensor([22, 16, 25])
```
若多次运行该脚本，会发现打印输出的顺序是非确定性的。由于应用运行在分布式环境中，我们无法控制命令执行的精确顺序——唯一能保证的是，全归约操作完成后，所有独立进程会持有完全相同（按位一致）的结果张量。

我们再仔细分析上述脚本：
- `mp.spawn` 函数会启动 `nprocs` 个进程，每个进程都会执行传入的 `fn` 函数并传入指定参数。
- 此外，`fn` 函数的调用格式为 `fn(rank, *args)`，其中 `rank` 是工作进程的索引（取值范围为 `0` 到 `nprocs-1`）。因此，我们的 `distributed_demo` 函数必须将该整数 `rank` 作为第一个位置参数。
- 我们还传入了 `world_size`，表示工作进程的总数。

每个工作进程都属于一个进程组（**process group**），通过 `dist.init_process_group` 初始化。进程组是指多个工作进程的集合，它们通过一个共享的主节点（master）进行协调和通信。主节点由其IP地址和端口定义，且主节点运行的是 `rank=0` 的进程。全归约等集合通信操作会作用于进程组中的所有进程。

在本示例中，我们使用 `gloo` 后端初始化进程组，但PyTorch还支持其他后端：
- `nccl` 后端：基于 NVIDIA 的 NCCL 集合通信库，对 CUDA 张量的性能通常更优，但仅支持配备 GPU 的机器。
- `gloo` 后端：可运行在仅含 CPU 的机器上。

实用经验法则：分布式 GPU 训练使用 NCCL 后端，分布式 CPU 训练和/或本地开发使用 Gloo 后端。本示例选择 Gloo 是为了支持在仅含 CPU 的机器上进行本地执行和开发。

运行多 GPU 任务时，需确保不同 `rank` 对应不同的 GPU。实现方式有两种：
- 在 `setup` 函数中调用 `torch.cuda.set_device(rank)`，使得 `tensor.to("cuda")` 会自动将张量移动到指定GPU。
- 显式创建每个 `rank` 对应的设备字符串（例如 `device = f"cuda:{rank}"`），并将其作为数据移动的目标设备（例如 `tensor.to(f"cuda:{rank}"`)）。

**术语定义**

在本作业的后续部分（以及你可能在网上看到的其他资源中），你会遇到PyTorch分布式通信相关的以下术语。尽管本作业聚焦于单节点、多进程分布式训练，但这些术语对理解通用分布式训练也很有帮助：
- 节点（node）：网络中的一台机器。
- 工作进程（worker）：参与分布式训练的程序实例。在本作业中，每个工作进程对应一个独立进程，因此我们会交替使用“工作进程（worker）”“进程（process）”。但在实际场景中，一个工作进程可能包含多个进程（例如用于加载训练数据），因此这些术语并非始终等价。
- 全局进程数（world size）：进程组中工作进程的总数。
- 全局序号（global rank）：用于唯一标识进程组中某个工作进程的整数ID（取值范围为 0 到 world_size-1）。例如，当全局进程数为2时，一个进程的全局序号为0（主进程），另一个为1。
- 本地进程数（local world size）：当应用跨多个节点运行时，本地进程数指某一节点上本地运行的工作进程数。例如，若在2个节点上各启动4个工作进程，则全局进程数为8，本地进程数为4。注意：单节点运行时，本地进程数与全局进程数相等。
- 本地序号（local rank）：用于唯一标识某台机器上本地工作进程索引的整数ID（取值范围为 0 到 local_world_size-1）。例如，若在2个节点上各启动4个进程，则每个节点上的工作进程本地序号为0、1、2、3。注意：单节点多进程分布式应用中，进程的本地序号与其全局序号相等。

![](../figures/fig10.png)

#### 2.1.1 分布式应用基准测试的最佳实践
在本部分作业中，你需要通过基准测试分布式应用，以更好地理解通信带来的开销。以下是一些最佳实践：
- 尽可能在同一台机器上运行基准测试，以确保对比的可控性。
- 在对目标操作计时前，先执行几次热身步骤（warm-up steps）——这对NCCL通信调用尤为重要，通常5次热身迭代即可。
- 在GPU上进行基准测试时，调用 `torch.cuda.synchronize()` 等待 CUDA 操作完成。注意：即使调用 `async_op=False` 的通信操作（表示操作在 GPU 上排队后返回，而非通信实际完成后返回），也需要执行该同步操作。
- 不同序号（rank）的计时结果可能略有差异，因此通常会汇总所有序号的测量结果以提高估计准确性。你可以使用全收集（all-gather）集合操作（特别是 `dist.all_gather` 对象函数）收集所有序号的结果。
- 通常在本地使用 Gloo 后端（CPU）调试，然后根据具体问题需求，使用 NCCL 后端（GPU）进行基准测试。切换后端只需修改 `init_process_group` 调用和张量设备转换逻辑。

**Problem（distributed_communication_single_node）：5分**
编写脚本，基准测试单节点多进程环境下全归约（all-reduce）操作的运行时间。上述示例代码可作为合理起点。尝试调整以下设置：
- 后端+设备类型：Gloo+CPU、NCCL+GPU。
- 全归约数据大小：float32类型张量，大小分别为1MB、10MB、100MB、1GB。
- 进程数：2、4或6个进程。

资源要求：最多使用6块GPU。每次基准测试运行时间应不超过5分钟。

交付物：对比不同设置的图表和/或表格，附加2-3句话的说明，阐述你的结果以及对各因素相互作用的思考。

**实验结果分析**
根据提供的基准测试结果，我们可以整理出以下数据表格：
|  进程数   | 	内存大小 (MB)  | 	平均时间 (ms)  |  带宽 (Gbps)  |
|  ----  | ----  | ----  | ----  |
| 2  | 1 | 0.19 | 88.23 |
| 2  | 10 | 1.48 | 113.34 |
| 2  | 100 | 14.61 | 114.82 |
| 2  | 1024 | 145.71 | 117.91 |
| 4  | 1 | 0.44 | 37.78 |
| 4  | 10 | 3.90 | 43.03 |
| 4  | 100 | 36.35 | 46.16 |
| 4  | 1024 | 367.49 | 46.75 |
| 6  | 1 | 0.51 | 32.71 |
| 6  | 10 | 4.59 | 36.58 |
| 6  | 100 | 42.52 | 39.46 |
| 6  | 1024 | 381.56 | 45.03 |

**关键发现**
1. **带宽随数据量增大而提升**：对于相同的进程数，随着数据量从1MB增加到1GB，有效带宽显著提升。这反映了通信启动开销（latency）的影响在数据量较小时更明显。
2. **进程数增加导致带宽下降**：在相同数据量下，2进程配置始终获得最高带宽，6进程配置带宽最低。例如，对于1GB数据，2进程带宽为117.91 Gbps，而6进程降至45.03 Gbps，降幅达62%。
3. **扩展性瓶颈明显**：进程数从2增加到6时，小数据量（1MB）的带宽下降了63%，大数据量（1GB）下降了62%，表明通信开销随进程数增加而显著增大。



