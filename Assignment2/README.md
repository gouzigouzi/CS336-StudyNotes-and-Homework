# Assignment 2 (systems): Systems and Parallelism

## 1 课程作业整体介绍

**你将实现的内容：**

1. **基准测试与性能分析工具**
2. **Flash Attention 2 Triton 内核**
3. **分布式数据并行训练**
4. **优化器状态分片**

在作业的第一部分，我们将学习如何优化 Transformer 模型性能，以充分利用 GPU 资源。会先对模型进行性能分析，了解其在正向传播和反向传播过程中时间与内存的消耗分布，然后通过自定义 GPU 内核优化自注意力机制，使其比 PyTorch 的原生实现更快。在作业的后续部分，我们将进一步探索多 GPU 训练方案。

### 1.1 性能分析与基准测试
在进行任何优化之前，先对程序进行性能分析至关重要——这能帮助我们了解资源（如时间和内存）在各个部分的消耗情况。否则，我们可能会耗费精力优化那些对整体性能影响甚微的模块，最终无法实现可量化的端到端性能提升。

我们将实现三种性能评估方式：（a）使用 Python 标准库进行简单的端到端基准测试，计时正向传播和反向传播过程；（b）借助 NVIDIA Nsight Systems 工具分析计算性能，了解时间在 CPU 和 GPU 各项操作中的分布；（c）分析内存使用情况。

#### 1.1.1 环境配置——导入基础 Transformer 模型
首先确保你能加载上一次作业实现的模型。上一次作业中，我们将模型封装为 Python 包，以便后续导入使用。我们已在 `./cs336-basics` 文件夹中提供了该模型的官方实现，并在 `pyproject.toml` 文件中配置了相关路径。与往常一样，通过 `uv run [command]` 命令运行代码时，uv 会自动定位这个本地的 `cs336-basics` 包。若你想使用自己实现的模型，可修改 `pyproject.toml` 文件，使其指向你的自定义包。

#### 1.1.2 模型规模定义
在本次作业中，我们将通过基准测试和性能分析不同规模的模型，以更好地理解性能变化规律。为了直观感受规模对性能的影响，我们将使用以下模型配置。所有模型的词汇表大小均为 10,000，批量大小（batch size）为 4，仅上下文长度（context length）不同。本次作业（及后续作业）需要呈现大量表格形式的结果，我们强烈建议你通过代码自动生成报告中的表格——因为手动用 LaTeX 或 Markdown 格式化表格非常繁琐。你可使用 `pandas.DataFrame.to_latex()`、`pandas.DataFrame.to_markdown()` 函数，或根据自己偏好的表格格式编写自定义生成函数。下表为不同模型规模的详细参数：

|  规模   | 	d_model（模型维度）  | 	d_ff（前馈网络维度）  | 	num_layers（层数）  | 	num_heads（注意力头数）  |
|  ----  | ----  | ----  | ----  | ----  |
| small（小型）  | 768 | 3072 | 12 | 12 |
| medium（中型）  | 1024 | 4096 | 24 | 16 |
| large（大型）  | 1280 | 5120 | 36 | 20 |
| xl（超大）  | 1600 | 6400 | 48 | 25 |
| 2.7B（27 亿参数）  | 2560 | 10240 | 32 | 32 |

#### 1.1.3 端到端基准测试
现在我们将实现一个简单的性能评估脚本。由于需要测试模型的多种变体（如改变精度、替换层结构等），建议你的脚本支持通过命令行参数配置这些变体，以便后续快速运行测试。我们还强烈建议使用 Slurm 上的 `sbatch` 或 `submitit` 工具，对基准测试的超参数（如模型规模、上下文长度等）进行批量扫描，以提高迭代效率。

首先，我们对模型进行最简单的性能分析——计时正向传播和反向传播过程。由于仅需测量速度和内存，我们将使用随机权重和随机数据进行测试。

性能测量需要注意一些细节——常见的陷阱可能导致测量结果失真。对于 GPU 代码的基准测试，一个重要的注意点是 CUDA 调用的异步性：当你调用一个 CUDA 内核（例如 `torch.matmul`）时，函数调用会立即返回控制权给 CPU，而无需等待 GPU 完成矩阵乘法运算。这样一来，CPU 可以在 GPU 进行计算的同时继续执行其他操作，但这也意味着，直接测量 `torch.matmul` 调用的返回时间，并不能反映 GPU 实际执行该矩阵乘法的耗时。在 PyTorch 中，你可以调用 `torch.cuda.synchronize()` 等待所有 GPU 内核执行完成，从而更准确地测量 CUDA 内核的运行时间。基于这一点，我们来编写基础的性能分析框架。

**Problem (benchmarking_script): 4 points**
（a）编写一个脚本，对模型的正向传播和反向传播进行基础的端到端基准测试。具体来说，你的脚本应支持以下功能：
- 根据超参数（如层数）初始化模型；
- 生成一批随机数据；
- 先运行 w 次热身步骤（开始计时前），然后计时 n 次步骤的执行时间（可通过参数选择仅测试正向传播，或同时测试正向传播和反向传播）；计时时建议使用 Python 的 `timeit` 模块（例如 `timeit` 函数，或 `timeit.default_timer()`——该函数提供系统最高分辨率的时钟，比 `time.time()` 更适合基准测试）；
- 每步执行后调用 `torch.cuda.synchronize()`。
- 
交付物：一个能够根据给定超参数初始化基础 Transformer 模型、生成随机数据，并计时正向传播和反向传播过程的脚本。

（b）对 1.1.2 节中描述的所有模型规模进行正向传播和反向传播计时。使用 5 次热身步骤，计算 10 次测量步骤的平均时间和标准差。正向传播耗时多久？反向传播耗时多久？测量结果的变异性大吗？还是标准差较小？

交付物：1-2 句话的回答，包含你的计时结果。

（c）基准测试的一个常见问题是未执行热身步骤。请在不进行热身的情况下重复上述分析，这会对结果产生什么影响？你认为原因是什么？另外，尝试使用 1 或 2 次热身步骤运行脚本，结果为何可能仍然不同？

答：如果不热身，前向传播和反向传播都明显比热身5次慢

#### 1.1.4 Nsight Systems Profiler
端到端基准测试无法告诉我们模型在正向传播和反向传播过程中时间与内存的具体消耗分布，因此无法精准定位优化机会。要了解程序在每个组件（如函数）上的耗时，我们需要使用性能分析工具（Profiler）。执行性能分析工具会通过在函数开始和结束时插入“探针”来检测代码，从而提供函数级别的详细执行统计信息（如调用次数、平均耗时、累计耗时等）。

标准的 Python 性能分析工具（如 `CProfile`）无法分析 CUDA 内核，因为这些内核是在 GPU 上异步执行的。幸运的是，NVIDIA 提供了一个可通过命令行使用的性能分析工具 `nsys`，我们已为你预装。在本部分作业中，你将使用 `nsys` 分析 Transformer 模型的运行时间。`nsys` 的使用非常简单：只需在运行上一部分编写的 Python 脚本前加上 `nsys profile` 前缀即可。例如，要分析脚本 `benchmark.py` 并将输出保存到文件 `result.nsys.rep`，可执行以下命令：
`~$ uv run nsys profile -o result python benchmark.py`

之后，你可以在本地机器上使用 `NVIDIA Nsight Systems` 桌面应用查看性能分析结果。在性能分析结果的 “CUDA API” 行中选中某个特定的 CUDA API 调用（CPU 端），会在 “CUDA HW” 行中高亮显示所有对应的内核执行（GPU 端）。

我们鼓励你尝试 `nsys profile` 的各种命令行选项，以了解其功能。值得注意的是，通过 `--python-backtrace=cuda` 选项，你可以获取每个 CUDA API 调用的 Python 回溯信息（但这可能会增加额外开销）。你还可以使用 NVTX 范围（NVTX ranges）对代码进行标注，这些标注会在性能分析结果的“NVTX”行中以块的形式显示，包含所有相关的 CUDA API 调用及关联的内核执行。具体来说，你应该使用 NVTX 范围忽略基准测试脚本中的热身步骤（通过在性能分析结果的“NVTX”行中设置过滤条件）。你还可以通过以下方式标注自注意力层的不同部分，从而分离出模型正向传播和反向传播过程中对应的内核，甚至定位自注意力层各组件对应的内核：
```python
import torch.cuda.nvtx as nvtx

@nvtx.range("缩放点积注意力")
def annotated_scaled_dot_product_attention(
    # Q、K、V、掩码等参数
):
    with nvtx.range("计算注意力分数"):
        # 计算 Q 和 K 之间的注意力分数
    with nvtx.range("计算 softmax"):
        # 计算注意力分数的 softmax
    with nvtx.range("最终矩阵乘法"):
        # 计算输出投影
    return
```

你可以通过以下方式，在基准测试脚本中用带标注的实现替换原始实现：
`cs336_basics.model.scaled_dot_product_attention = annotated_scaled_dot_product_attention
`

最后，你可以在运行 nsys 时添加 --pytorch 命令行选项，自动为 PyTorch C++ API 的调用添加 NVTX 范围标注。

**任务（nsys 性能分析）：5 分**
使用 nsys 对表 1 中所有模型规模，以及context lengths为 128、256、512 和 1024 的情况，分别进行正向传播、反向传播和优化器步骤的性能分析（对于部分大型模型，某些上下文长度可能会导致内存不足，这种情况请在报告中注明）。

（a）正向传播的总耗时是多少？与之前使用 Python 标准库测量的结果是否一致？

交付物：测试了context lengths = 256，表格中的小模型'small': {'d_model': 768, 'd_ff': 3072, 'num_layers': 12, 'num_heads': 12},，一次正向传播耗时约等于 31ms，和使用python库测量的基本一致。

（b）正向传播过程中，累计 GPU 耗时最长的 CUDA 内核是什么？在模型的单次正向传播中，该内核被调用多少次？当同时进行正向传播和反向传播时，耗时最长的内核是否相同？（提示：查看“Stats Systems View”下的“CUDA GPU Kernel Summary”，并使用 NVTX 范围过滤，以确定哪些模型组件对应哪些内核。）

（c）尽管绝大部分浮点运算 FLOPs 发生在矩阵乘法中，但你会注意到其他一些内核仍占用了相当比例的总运行时间。在前向传播过程中，除了矩阵乘法之外，你观察到还有哪些内核占用了不可忽视的CUDA运行时间？

答：除了矩阵乘法，`at::native::unrolled_elementwise_kernal`和`at::native::reduce_kernal` ，也都很耗时，分别对应激活函数、加减法 和 求和、求均值等运算操作。

（d）对完整的训练步骤（即正向传播、计算损失、执行反向传播，最后执行优化器步骤——与实际训练过程一致）进行性能分析。与仅进行推理（仅正向传播）相比，矩阵乘法占用的时间比例有何变化？其他内核的时间比例变化如何？

答：与纯推理相比，完整训练过程中矩阵乘法（GEMM）内核的时间占比下降，而逐元素操作（如梯度缩放、激活函数导数、优化器更新）和内存操作的时间占比显著上升。这反映出训练不仅包含前向计算，还引入了大量反向传播和参数更新相关的非 GEMM 计算，使得整体计算模式更加多样化，GEMM 不再是绝对主导。

（e）比较正向传播过程中，自注意力层内 softmax 操作与矩阵乘法操作的运行时间。两者的运行时间差异与浮点运算量（FLOPs）差异相比，情况如何？

答：**Softmax 的实际运行开销远高于其 FLOPs 所暗示的水平**。尽管其算术复杂度很低（仅占 matmul FLOPs 的 0.14%），但其运行时间却达到了 matmul 的 23.8%（161.9 / 681.9）。这说明在自注意力机制中，**softmax 是一个“低 FLOPs 但高延迟”的瓶颈操作**，优化其内存访问模式或使用近似方法（如 FlashAttention）能显著提升整体效率。

#### 1.1.5 混合精度
截至目前，模型均使用 FP32（单精度浮点数）进行计算——所有模型参数和激活值均为 `torch.float32` 数据类型。然而，现代 NVIDIA GPU 配备了专门的 GPU 核心（Tensor Cores），可加速低精度下的矩阵乘法运算。例如，NVIDIA A100 的技术规格显示，其 FP32 精度下的最大吞吐量为 19.5 TFLOP/秒，而在 FP16（半精度浮点数）或 BF16（脑浮点数）精度下，最大吞吐量可显著提升至 312 TFLOP/秒。因此，**使用低精度数据类型有助于加快训练和推理速度**。

然而，将模型直接转换为低精度格式可能会导致模型精度下降。例如，实际应用中许多梯度值往往过小，无法用FP16（半精度浮点数）表示，因此在直接使用FP16精度训练时会变为零。为解决这一问题，使用FP16训练时通常会采用**损失缩放技术**——只需将损失乘以一个缩放因子，增大梯度幅度以避免其被置零。此外，FP16的动态范围小于FP32（单精度浮点数），可能会导致溢出，表现为损失值变为NaN（非数字）。全bfloat16（简称BF16）训练通常更稳定（因为BF16与FP32的动态范围相同），但与FP32相比仍可能影响模型的最终性能。

为充分利用低精度数据类型带来的速度提升，**混合精度训练**成为常用方案。在PyTorch中，这一功能通过 `torch.autocast` 上下文管理器实现。在该模式下，部分操作（如矩阵乘法）会以低精度数据类型执行，而其他需要FP32完整动态范围的操作（如累加和归约）则保持原有精度不变。例如，以下代码会在正向传播过程中自动识别应采用低精度执行的操作，并将这些操作转换为指定的数据类型：
```python
model: torch.nn.Module = ...  # 例如你的Transformer模型
dtype: torch.dtype = ...      # 例如torch.float16
x: torch.Tensor = ...         # 输入数据
with torch.autocast(device="cuda", dtype=dtype):
    y = model(x)
```
如前所述，即使被累加的张量本身已被降精度转换，累加操作仍建议保持高精度，以下练习将帮助你理解其原因。

**问题（mixed_precision_accumulation）：1分**
运行以下代码并评论结果（的准确性）。
```python
import torch

s = torch.tensor(0, dtype=torch.float32)
for i in range(1000):
    s += torch.tensor(0.01, dtype=torch.float32)
print(s) # tensor(10.0001)

s = torch.tensor(0, dtype=torch.float16)
for i in range(1000):
    s += torch.tensor(0.01, dtype=torch.float16)
print(s) # tensor(9.9531, dtype=torch.float16)

s = torch.tensor(0, dtype=torch.float32)
for i in range(1000):
    s += torch.tensor(0.01, dtype=torch.float16)
print(s) # tensor(10.0021)

s = torch.tensor(0, dtype=torch.float32)
for i in range(1000):
    x = torch.tensor(0.01, dtype=torch.float16)
    s += x.type(torch.float32)
print(s) # tensor(10.0021)
```
接下来，我们将先在简单模型上应用混合精度以建立直观认知，再将其应用到基准测试脚本中。

**问题（benchmarking_mixed_precision）：2分**
(a) 考虑以下模型
```python
class ToyModel(nn.Module):
    def __init__(self, in_features: int, out_features: int):
        super().__init__()
        self.fc1 = nn.Linear(in_features, 10, bias=False)
        self.ln = nn.LayerNorm(10)
        self.fc2 = nn.Linear(10, out_features, bias=False)
        self.relu = nn.ReLU()
    
    def forward(self, x):
        x = self.relu(self.fc1(x))
        x = self.ln(x)
        x = self.fc2(x)
        return x
```
假设我们在GPU上训练该模型，且模型参数初始为FP32精度。我们希望使用FP16的自动混合精度训练，请指出以下组件的数据类型：
- 自动混合精度上下文（autocast context）内的模型参数
- 第一个前馈层（ToyModel.fc1）的输出
- 层归一化（ToyModel.ln）的输出
- 模型的预测对数概率（logits）
- 损失（loss）
- 模型的梯度（gradients）

提交要求：列出上述每个组件的数据类型。
- 自动混合精度上下文（autocast context）内的模型参数 : FP32
- 第一个前馈层（ToyModel.fc1）的输出 : FP16
- 层归一化（ToyModel.ln）的输出 : FP32
- 模型的预测对数概率（logits） : FP32
- 损失（loss） : FP32
- 模型的梯度（gradients） : FP32

(b) 你应已发现，FP16混合精度自动转换对层归一化层的处理与前馈层不同。层归一化的哪些部分对混合精度敏感？若使用BF16替代FP16，是否仍需对层归一化进行特殊处理？为什么？

答：层归一化涉及方差计算，平方操作在FP16下容易溢出。对 FP16：LayerNorm 需要用 FP32 计算以保证稳定（PyTorch AMP 默认如此）。对 BF16：通常可以直接使用 BF16 计算，无需特殊强制 FP32，因为其数值范围已足够大。

(c）修改你的基准测试脚本，使其支持可选地使用BF16混合精度运行模型。对1.1.2节中描述的每种语言模型规模，分别测试混合精度开启与关闭时的正向传播和反向传播时间。对比全精度与混合精度的结果，并评论模型规模变化时的趋势。你可能会发现 `nullcontext` 空操作上下文管理器很有用。

提交要求：2-3句话的回复，包含计时结果和评论。
对于参数量小的模型 small 和 medium，全精度前向传播和反向传播都更快，但对于 large 模型（或者更大），混合精度更快。

#### 1.1.6 内存分析
到目前为止，我们关注的是计算性能。现在我们将转向内存——语言模型训练和推理中的另一项核心资源。PyTorch内置了强大的内存分析器，可跟踪随时间变化的内存分配情况。

要使用内存分析器，可按以下方式修改基准测试脚本：
```python
# 基准测试脚本中的预热阶段
...
# 开始记录内存历史
torch.cuda.memory._record_memory_history(max_entries=1000000)
# 你要分析的代码部分
...
# 保存Pickle文件，供PyTorch在线工具加载
torch.cuda.memory._dump_snapshot("memory_snapshot.pickle")
# 停止记录历史
torch.cuda.memory._record_memory_history(enabled=None)
```
运行后会生成 `memory_snapshot.pickle` 文件，可将其加载到[在线工具](https://pytorch.org/memory_viz)中。该工具将展示整体内存使用时间线，以及每个单独的内存分配（包括分配大小和指向代码来源的调用栈）。使用时，在浏览器中打开上述链接，将 Pickle 文件拖放到页面即可。

请使用PyTorch分析器分析模型的内存使用情况。

**问题（memory_profiling）：4分**
分析表1中2.7B模型在上下文长度为128、256和512时的正向传播、反向传播和优化器步骤。

(a) 在你的分析脚本中添加选项，使模型能够通过内存分析器运行。可复用之前的部分架构（如启用混合精度、加载特定模型规模等）。然后运行脚本，获取2.7B模型仅推理（仅正向传播）或完整训练步骤（正向传播、反向传播、优化器步骤）的内存分析结果。内存时间线呈现何种特征？能否根据观察到的峰值判断当前运行阶段？
提交要求：两张来自memory_viz工具的2.7B模型“活跃内存时间线”图片（一张为正向传播，一张为完整训练步骤），以及2-3句话的回复。

(b) 不同上下文长度下，仅正向传播的峰值内存使用量是多少？完整训练步骤的峰值内存使用量又是多少？
提交要求：一个表格，每个上下文长度对应两个数值。

(c) 分别获取2.7B模型在混合精度下仅正向传播和完整优化器步骤的峰值内存使用量。混合精度是否会显著影响内存使用？
提交要求：2-3句话的回复。

(d) 考虑2.7B模型。在参考超参数下，Transformer残差流中激活张量的单精度大小是多少？以MB为单位（即将字节数除以1024²）。
提交要求：1-2句话的回复，包含推导过程。

(e) 仔细观察2.7B模型正向传播的内存快照“Active Memory Timeline”。降低“Detail”（细节）级别时，工具会隐藏对应级别以下的最小分配（例如，将“Detail”设为10%仅显示最大的10%分配）。此时显示的最大分配大小是多少？通过调用栈能否判断这些分配来自何处？
提交要求：1-2句话的回复。

### 1.2 用 FlashAttention-2 优化注意力机制

#### 1.2.1 PyTorch注意力机制基准测试
你的分析结果可能表明，注意力层在内存和计算方面存在优化空间。从高层来看，注意力操作包括三次矩阵乘法和一次 softmax 激活：
1. 计算查询（Q）、键（K）的点积以得到注意力分数；
2. 对注意力分数应用 softmax 归一化；
3. 将归一化后的分数与值（V）进行矩阵乘法，得到最终注意力输出。

朴素的注意力实现需要为每个批次/头元素存储形状为 seq_len×seq_len（序列长度×序列长度）的注意力分数矩阵。当序列长度较长时，该矩阵会变得极大，导致长输入或长输出任务出现内存不足错误。我们将基于 FlashAttention-2 论文实现一个注意力内核，通过分块（tile）计算注意力，避免显式生成 seq_len×seq_len 的注意力分数矩阵，从而支持更长的序列长度。

**问题（pytorch_attention）：2分**
(a) 基准测试不同规模下的注意力实现。编写脚本完成以下任务：

(i) 固定批次大小（batch size）为8，不使用多头注意力（即移除头维度）；
(ii) 遍历模型维度 $d_{model}$ 的取值集合[16, 32, 64, 128]与序列长度的取值集合[256, 1024, 4096, 8192, 16384]的笛卡尔积；
(iii) 生成对应大小的随机输入Q、K、V；
(iv) 计时100次正向传播；
(v) 测量反向传播开始前的内存使用量，并计时100次反向传播；
(vi) 确保进行预热，并在每次正向/反向传播后调用torch.cuda.synchronize()。

报告上述配置下的计时结果（或内存不足错误）。在何种规模下会出现内存不足错误？选择一个你发现的最小内存不足配置，计算注意力机制的内存使用量（可使用第一次作业中 Transformer 的内存使用公式）。反向传播的内存节省量如何随序列长度变化？你会如何消除这部分内存开销？
提交要求：一个包含计时结果的表格、内存使用量的计算过程，以及1-2段的回复。


